# JARVIS - Multimodal Agentic AI Assistant

<div align="center">

![JARVIS](https://img.shields.io/badge/JARVIS-AI%20Assistant-blue)
![Python](https://img.shields.io/badge/Python-3.12+-green)
![LLaMA](https://img.shields.io/badge/LLaMA-3.1%208B-orange)
![License](https://img.shields.io/badge/License-MIT-yellow)

**Asisten AI multimodal berbasis agentic architecture dengan LLaMA 3.1 8B**

[Features](#features) â€¢ [Architecture](#architecture) â€¢ [Installation](#installation) â€¢ [Usage](#usage) â€¢ [Documentation](#documentation)

</div>

---

## ğŸ“‹ Overview

JARVIS adalah asisten AI multimodal berbasis agentic architecture yang dibangun menggunakan Large Language Model lokal melalui Ollama dengan model LLaMA 3.1 8B. Sistem ini mampu berinteraksi melalui teks dan suara, melakukan reasoning dan perencanaan tugas, serta mengeksekusi aksi nyata melalui tool calling manual dan Model Context Protocol (MCP).

### ğŸ¯ Key Features

- **ğŸ¤– Agentic Reasoning**: Plan-Act-Observe loop untuk task execution
- **ğŸ—£ï¸ Multimodal**: Input/output teks dan suara
- **ğŸ”§ Tool Calling**: Manual JSON-based tool execution
- **ğŸ”Œ MCP Integration**: Modular external system integration
- **ğŸ’¾ Memory System**: Short-term, long-term, dan semantic memory (RAG)
- **ğŸŒ Multi-channel**: Web UI, Telegram, Voice interface
- **ğŸ”’ Privacy-First**: Local LLM dan processing

## ğŸ—ï¸ Architecture

```
User (Web UI / Voice / Telegram)
    â†“
Input Normalization
    â†“
LLaMA 3.1 8B (Ollama)
    â†“
Agent Core (Plan-Act-Observe)
    â†“
Tool Router (MCP Client)
    â†“
MCP Servers (Calendar, Gmail, OS, DB, Telegram)
    â†“
Observation & Response
```

### Core Components

- **Agent Core**: Implementasi agentic loop dengan planning, execution, dan observation
- **LLM Integration**: Ollama client untuk LLaMA 3.1 8B
- **MCP Client**: Komunikasi dengan MCP servers
- **Memory System**: Three-tier memory architecture
- **API Layer**: REST dan WebSocket endpoints

## ğŸš€ Installation

### Prerequisites

- Python 3.12 or higher
- [Ollama](https://ollama.ai/) installed and running
- [uv](https://github.com/astral-sh/uv) package manager (recommended)

### Setup

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd jarvis
   ```

2. **Install dependencies**
   ```bash
   # Using uv (recommended)
   uv sync
   
   # Or using pip
   pip install -e .
   ```

3. **Pull Ollama models**
   ```bash
   ollama pull llama3.1:8b
   ollama pull nomic-embed-text
   ```

4. **Configure environment**
   ```bash
   cd backend
   cp .env.example .env
   # Edit .env with your configuration
   ```

5. **Initialize database**
   ```bash
   cd backend
   python -c "from app.models.database import init_db; init_db()"
   ```

## ğŸ’» Usage

### Starting the Backend

```bash
cd backend
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

Or using Python directly:
```bash
cd backend
python -m app.main
```

### API Endpoints

- **Health Check**: `GET /health`
- **Chat**: `POST /api/chat/send`
- **WebSocket**: `WS /api/ws/chat`
- **Voice STT**: `POST /api/voice/stt`
- **Voice TTS**: `POST /api/voice/tts`

### Example Chat Request

```bash
curl -X POST http://localhost:8000/api/chat/send \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What is my schedule today?",
    "conversation_id": "optional-uuid"
  }'
```

### WebSocket Chat

```javascript
const ws = new WebSocket('ws://localhost:8000/api/ws/chat');

ws.onopen = () => {
  ws.send(JSON.stringify({
    type: 'chat',
    message: 'Hello JARVIS!',
    conversation_id: 'uuid'
  }));
};

ws.onmessage = (event) => {
  const data = JSON.parse(event.data);
  console.log('Response:', data);
};
```

## ğŸ“š Documentation

### Project Structure

```
jarvis/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ agent/          # Agentic reasoning
â”‚   â”‚   â”œâ”€â”€ api/            # REST & WebSocket endpoints
â”‚   â”‚   â”œâ”€â”€ llm/            # Ollama integration
â”‚   â”‚   â”œâ”€â”€ mcp/            # MCP client
â”‚   â”‚   â”œâ”€â”€ models/         # Database models
â”‚   â”‚   â”œâ”€â”€ config.py       # Configuration
â”‚   â”‚   â””â”€â”€ main.py         # FastAPI app
â”‚   â”œâ”€â”€ .env.example        # Environment template
â”‚   â””â”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ mcp-servers/            # MCP server implementations
â”œâ”€â”€ frontend/               # Next.js web UI (coming soon)
â”œâ”€â”€ pyproject.toml          # Project configuration
â””â”€â”€ README.md               # This file
```

### Agent Loop

JARVIS menggunakan **Plan-Act-Observe** loop:

1. **Plan**: LLM menganalisis request dan membuat execution plan
2. **Act**: Agent mengeksekusi tools yang diperlukan via MCP
3. **Observe**: LLM menganalisis hasil dan menentukan next action

### Available Tools

- **Memory**: save_conversation, get_user_preferences, log_interaction
- **Vector DB**: semantic_search, retrieve_context, store_embedding
- **Telegram**: send_message, get_updates, send_notification
- **Calendar**: list_events, create_event, update_event, delete_event
- **Gmail**: list_emails, read_email, create_draft, send_email
- **Windows OS**: open_application, run_powershell, manage_files
- **Voice**: transcribe_audio, synthesize_speech

## ğŸ”§ Configuration

Edit `backend/.env` to configure:

- **Ollama**: URL, model, timeout
- **Database**: SQLite/PostgreSQL URL, ChromaDB path
- **External APIs**: Google Calendar, Gmail, Telegram tokens
- **MCP Servers**: URLs for each MCP server
- **API**: Host, port, CORS origins

## ğŸ§ª Development

### Running Tests

```bash
pytest backend/tests/
```

### Code Style

```bash
# Format code
black backend/

# Lint
ruff backend/
```

## ğŸ“ Roadmap

- [x] Phase 1: Core infrastructure & agent loop
- [ ] Phase 2: Memory system implementation
- [ ] Phase 3: MCP servers development
- [ ] Phase 4: Voice interface (STT/TTS)
- [ ] Phase 5: Web frontend (Next.js)
- [ ] Phase 6: External integrations
- [ ] Phase 7: Testing & documentation
- [ ] Phase 8: Deployment

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai/) for local LLM inference
- [LLaMA](https://ai.meta.com/llama/) by Meta AI
- [FastAPI](https://fastapi.tiangolo.com/) for the web framework
- [Model Context Protocol](https://modelcontextprotocol.io/) for tool integration

---

<div align="center">

**Built with â¤ï¸ using LLaMA 3.1 8B and Ollama**

</div>
